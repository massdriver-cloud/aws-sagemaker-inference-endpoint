schema: draft-07
name: aws-sagemaker-inference-endpoint
description: AWS SageMaker Inference Endpoint for hosting an AI model
source_url: github.com/massdriver-cloud/aws-sagemaker-inference-endpoint
access: public
type: infrastructure

params:
  required:
    - endpoint_config
  properties:
    endpoint_config:
      title: Endpoint Configuration
      type: object
      required:
        - primary_container
        - instance_type
        - instance_count
      properties:
        primary_container:
          title: Primary Container
          type: object
          required:
            - model_data
            - ecr_image
          properties:
            model_data:
              title: Model Data S3 URI
              description: "The S3 URI of the model data. (e.g. s3://bucket-name/model.tar.gz)"
              type: string
              pattern: ^s3:\/\/.+\.tar\.gz$
              message:
                pattern: Must be a valid S3 URI ending in .tar.gz
            ecr_image:
              title: ECR Image URI
              description: "The ECR Image URI. (e.g. 	763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.1.0-gpu-py310-cu118-ubuntu20.04-ec2)"
              type: string
              pattern: ^\d+\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com(\.cn)?\/[a-z0-9-]+:[a-z0-9-.]+$
              message:
                pattern: Must be a valid ECR Image URI
        instance_type:
          title: SageMaker Instance Type
          description: Instance type to use for the SageMaker endpoint
          type: string
          oneOf:
            - title: ml.t3.medium - Comparable to Basic CPUs
              const: ml.t3.medium
            - title: ml.t3.large - Comparable to Basic CPUs with more Memory
              const: ml.t3.large
            - title: ml.m5.large - General Purpose (2 vCPU, 8 GiB Memory)
              const: ml.m5.large
            - title: ml.m5.xlarge - General Purpose (4 vCPU, 16 GiB Memory)
              const: ml.m5.xlarge
            - title: ml.c5.2xlarge - High CPU (8 vCPU, 16 GiB Memory)
              const: ml.c5.2xlarge
            - title: ml.c5.4xlarge - High CPU (16 vCPU, 32 GiB Memory)
              const: ml.c5.4xlarge
            - title: ml.p2.xlarge - GPU Compute (1 K80 GPU)
              const: ml.p2.xlarge
            - title: ml.p2.8xlarge - High-Performance GPUs (8 K80 GPUs)
              const: ml.p2.8xlarge
            - title: ml.p3.2xlarge - Advanced GPU Compute (1 V100 GPU)
              const: ml.p3.2xlarge
            - title: ml.p3.8xlarge - High-Performance GPUs (4 V100 GPUs)
              const: ml.p3.8xlarge
            - title: ml.g4dn.xlarge - GPU Optimized (1 T4 GPU)
              const: ml.g4dn.xlarge
            - title: ml.g4dn.12xlarge - GPU Optimized for Graphics (4 T4 GPUs)
              const: ml.g4dn.12xlarge
            - title: ml.inf1.xlarge - Inferentia Chips for High Performance Inference
              const: ml.inf1.xlarge
            - title: ml.inf1.6xlarge - High Performance Inferentia Chips
              const: ml.inf1.6xlarge
            - title: ml.c5.9xlarge - Very High CPU (36 vCPU, 72 GiB Memory)
              const: ml.c5.9xlarge
            - title: ml.g5.large - NVIDIA A10G GPU Optimized (1 vCPU, 8 GiB Memory)
              const: ml.g5.large
            - title: ml.g5.xlarge - NVIDIA A10G GPU Optimized (4 vCPU, 16 GiB Memory)
              const: ml.g5.xlarge
            - title: ml.g5.2xlarge - NVIDIA A10G GPU Optimized (8 vCPU, 32 GiB Memory)
              const: ml.g5.2xlarge
            - title: ml.g5.4xlarge - NVIDIA A10G GPU Optimized (16 vCPU, 64 GiB Memory)
              const: ml.g5.4xlarge
            - title: ml.g5.8xlarge - NVIDIA A10G GPU Optimized (32 vCPU, 128 GiB Memory)
              const: ml.g5.8xlarge
            - title: ml.g5.12xlarge - High Performance NVIDIA A10G GPUs (48 vCPU, 192 GiB Memory)
              const: ml.g5.12xlarge
            - title: ml.g5.16xlarge - High Performance NVIDIA A10G GPUs (64 vCPU, 256 GiB Memory)
              const: ml.g5.16xlarge
            - title: ml.g5.24xlarge - High Performance NVIDIA A10G GPUs (96 vCPU, 384 GiB Memory)
              const: ml.g5.24xlarge
        instance_count:
          title: Initial Instance Count
          description: "Initial number of instances used for auto-scaling."
          type: integer
          minimum: 1

connections:
  required:
  - aws_authentication
  - s3_model_bucket
  - vpc
  properties:
    aws_authentication:
      $ref: massdriver/aws-iam-role
    s3_model_bucket:
      $ref: massdriver/aws-s3-bucket
    vpc:
      $ref: massdriver/aws-vpc

artifacts: {}

ui:
  ui:order:
    - primary_container
    - instance_type
    - instance_count
    - "*"
  primary_container:
    ui:order:
      - model_data
      - ecr_image
      - "*"
